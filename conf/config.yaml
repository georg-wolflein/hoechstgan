defaults:
  - _self_
  - dataset@dataset.input: h3342
  - dataset@dataset.outputs.B: default

name: experiment
wandb_id:
is_train: true
phase: train
initial_epoch: 0 # where to start counting epochs
load_checkpoint: # (epoch | latest | null) if specified, will load checkpoint (if epoch, will use initial_epoch's checkpoint) and continue training from there
data_folder: ${oc.env:DATA_FOLDER,/Users/georg/Desktop/Scratch.nosync}
checkpoints_dir: ${data_folder}/checkpoints
dataset:
  data_root: ${data_folder}/patches
  input:
    props: # key-value pairs to identify images
    transforms: []
    num_channels: 1
  outputs:
    B:
      props: # key-value pairs to identify images
      transforms: []
      num_channels: 1
    # C:
    #   props: # key-value pairs to identify images
    #   transforms: []
    #   num_channels: 1
  batch_size: 64
  shuffle: true
  num_threads: 8 # number of threads for loading data
  max_size: .inf # maximum size of the dataset per epoch
norm: batch
initialization: normal
initialization_scale: .02 # scaling factor
gpus: [4, 5, 6, 7]
learning_rate:
  initial: 0.0002
  policy: linear # learning rate policy: linear | step | plateau | cosine
  n_epochs_initial: 20 # number of epochs with the initial learning rate
  n_epochs_decay: 10 # number of epochs to linearly decay learning rate to zero
  decay_iters: 50 # multiply by a gamma every decay_iters iterations
beta1: .5
gan: pix2pix
discriminator:
  filters: 64
  layers: 3
  type: separate # separate | joint
generator:
  filters: 64
  dropout: true
  dropout_eval_mode: "dropout" # how to apply dropout at test time: identity | average | dropout
  outputs: [fake_B]
  encoders:
    - from: real_A
      to: latent_A
  decoders:
    - from: latent_A
      to: fake_B
  composites: []
loss:
  generator:
    ground_truth:
      l1: 100.0
      l2: 0.0
      kl: 0.0
    coefficient: 1. # TODO: try .5
  discriminator:
    coefficient: 1. # TODO: try .5
verbose: true
log_freq: 100 # log metrics every n iterations
visualize_freq: 5000 # log visualizations every n iterations, must be a multiple of log_freq
save_epoch_freq: 1
save_latest_freq: 5000
save_by_iter: false # if false, save only the latest model
num_epochs: ${sum:${learning_rate.n_epochs_initial},${learning_rate.n_epochs_decay}}
